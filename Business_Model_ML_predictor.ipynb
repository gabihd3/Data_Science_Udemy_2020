{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we were presented data from an Audiobooks company. There were 12 columns, where the first column was just the id of the customer and the last one was the targets columns (either 0s or 1s) and this target showed whether a customer bought again after 6 months of its first purchase. Therefore we were left with 10 predictors, or inputs, and around 14000 rows of data.\n",
    "\n",
    "Here we preprocess the data, divide it into train,validation and test and feed it into the model. We try to predict whether a customer will buy in the future, so that the ads are targeted to those customer who are potential regular buyers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_all = raw_csv_data[:,1:-1] #Disregard rows and omit column 0 and last column\n",
    "targets_all = raw_csv_data[:,-1] #Targets are found in the last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.12 Data targets are incredibly uneven (84%-0, 16%-1)\n"
     ]
    }
   ],
   "source": [
    "#Our targets are either 0 or 1\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "\n",
    "#To find the percentage of 0s with respect to 1s\n",
    "x =((targets_all.shape[0] - num_one_targets)/targets_all.shape[0])*100\n",
    "x1 = np.round(x,2)\n",
    "print(x1,'Data targets are incredibly uneven (84%-0, 16%-1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14084\n"
     ]
    }
   ],
   "source": [
    "print(targets_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to have a balance dataset for better training results\n",
    "#Therefore we will have to remove some input-target pairs\n",
    "\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "#Equal priors stands for a balanced input data            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)    \n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0) \n",
    "#np.delete removes from the chosen array, the object selected, in our case the indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of rows deleted: 9610\n"
     ]
    }
   ],
   "source": [
    "print('Amount of rows deleted:',targets_all.shape[0]-unscaled_inputs_equal_priors.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474 10 .10 are the inputs of the hidden layer (10 predictors)\n"
     ]
    }
   ],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "print(scaled_inputs.shape[0],\n",
    "scaled_inputs.shape[1],\n",
    "     '.10 are the inputs of the hidden layer (10 predictors)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# We want the data to be as randomly spread as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3579 447 448\n",
      "3579 10\n",
      "447 10\n",
      "448 10\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Train, validation and test data should be split as 80-10-10\n",
    "train_samples_count = int(samples_count*0.8)\n",
    "validation_samples_count = int(samples_count*0.1)\n",
    "test_samples_count = samples_count -(train_samples_count + validation_samples_count)\n",
    "print(train_samples_count,validation_samples_count,test_samples_count)\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "print(train_inputs.shape[0],train_inputs.shape[1])\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count + validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count + validation_samples_count]\n",
    "print(validation_inputs.shape[0],validation_inputs.shape[1])\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count:]\n",
    "print(test_inputs.shape[0],test_inputs.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788.0 3579 0.49958088851634536\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_targets),\n",
    "      train_samples_count, \n",
    "      np.sum(train_targets) / train_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.0 447 0.48769574944071586\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(validation_targets),\n",
    "      validation_samples_count, \n",
    "      np.sum(validation_targets) / validation_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.0 448 0.515625\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(test_targets),\n",
    "      test_samples_count,\n",
    "      np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.6229 - accuracy: 0.6449 - val_loss: 0.5082 - val_accuracy: 0.7450\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4705 - accuracy: 0.7759 - val_loss: 0.4317 - val_accuracy: 0.7673\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4195 - accuracy: 0.7784 - val_loss: 0.4020 - val_accuracy: 0.7830\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3957 - accuracy: 0.7835 - val_loss: 0.3901 - val_accuracy: 0.7919\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3792 - accuracy: 0.8002 - val_loss: 0.3736 - val_accuracy: 0.7897\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3690 - accuracy: 0.7963 - val_loss: 0.3728 - val_accuracy: 0.7919\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3615 - accuracy: 0.8069 - val_loss: 0.3630 - val_accuracy: 0.8121\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3580 - accuracy: 0.8008 - val_loss: 0.3535 - val_accuracy: 0.7987\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3507 - accuracy: 0.8075 - val_loss: 0.3528 - val_accuracy: 0.8143\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3464 - accuracy: 0.8120 - val_loss: 0.3509 - val_accuracy: 0.8188\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3453 - accuracy: 0.8120 - val_loss: 0.3464 - val_accuracy: 0.8255\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.3444 - accuracy: 0.8111 - val_loss: 0.3507 - val_accuracy: 0.8121\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.3458 - accuracy: 0.8092 - val_loss: 0.3444 - val_accuracy: 0.8188\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.3402 - accuracy: 0.8148 - val_loss: 0.3422 - val_accuracy: 0.8166\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.3351 - accuracy: 0.8150 - val_loss: 0.3409 - val_accuracy: 0.8121\n",
      "Epoch 16/100\n",
      "36/36 - 0s - loss: 0.3329 - accuracy: 0.8226 - val_loss: 0.3355 - val_accuracy: 0.8300\n",
      "Epoch 17/100\n",
      "36/36 - 0s - loss: 0.3308 - accuracy: 0.8209 - val_loss: 0.3371 - val_accuracy: 0.8412\n",
      "Epoch 18/100\n",
      "36/36 - 0s - loss: 0.3370 - accuracy: 0.8175 - val_loss: 0.3406 - val_accuracy: 0.8322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a40b9d150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation = 'softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', #another option is the classic sgd\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "max_epoch = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 2)\n",
    "\n",
    "model.fit(train_inputs,\n",
    "         train_targets,\n",
    "         batch_size = batch_size,\n",
    "         epochs = max_epoch,\n",
    "         callbacks = [early_stopping],\n",
    "         validation_data = (validation_inputs,validation_targets),\n",
    "         verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.7991\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(test_inputs,test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of around 80% is suboptimal, thus the model should have been improved prior to the testing. Anyway, out of 100 customers targeted, 80 are bound to be responsive by the ads campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0] *",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
